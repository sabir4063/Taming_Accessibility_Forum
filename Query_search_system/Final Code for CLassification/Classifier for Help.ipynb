{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabir Ismail\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pandas as pd\n",
    "tqdm_notebook.pandas()\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabir Ismail\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (4,5,6,7,8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Data/ready_for_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59465\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "# df = df[df.Type == 'original']\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Post</th>\n",
       "      <th>Type</th>\n",
       "      <th>Tag1</th>\n",
       "      <th>Help</th>\n",
       "      <th>Problem</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Clarification</th>\n",
       "      <th>Elaboration</th>\n",
       "      <th>...</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJR</th>\n",
       "      <th>JJS</th>\n",
       "      <th>RB</th>\n",
       "      <th>RBR</th>\n",
       "      <th>RBS</th>\n",
       "      <th>WRB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>2017 2 hello see announcement 2017 2 out machi...</td>\n",
       "      <td>original</td>\n",
       "      <td>tagged</td>\n",
       "      <td>Help</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>hi dave yes update available got it rosemarie</td>\n",
       "      <td>following</td>\n",
       "      <td>tagged</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>Answer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>change suspect rc1 canadate available day</td>\n",
       "      <td>following</td>\n",
       "      <td>tagged</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>Answer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>hi also updated latest nvda version help menu ...</td>\n",
       "      <td>following</td>\n",
       "      <td>tagged</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>sorry folk mick jamie working later last night...</td>\n",
       "      <td>following</td>\n",
       "      <td>tagged</td>\n",
       "      <td>undefined</td>\n",
       "      <td>undefined</td>\n",
       "      <td>Answer</td>\n",
       "      <td>undefined</td>\n",
       "      <td>Elaboration</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Title                                               Post       Type  \\\n",
       "0   1  2017.2  2017 2 hello see announcement 2017 2 out machi...   original   \n",
       "1   2  2017.2      hi dave yes update available got it rosemarie  following   \n",
       "2   3  2017.2          change suspect rc1 canadate available day  following   \n",
       "3   4  2017.2  hi also updated latest nvda version help menu ...  following   \n",
       "4   5  2017.2  sorry folk mick jamie working later last night...  following   \n",
       "\n",
       "     Tag1       Help    Problem     Answer Clarification  Elaboration ...   \\\n",
       "0  tagged       Help  undefined  undefined     undefined    undefined ...    \n",
       "1  tagged  undefined  undefined     Answer     undefined    undefined ...    \n",
       "2  tagged  undefined  undefined     Answer     undefined  Elaboration ...    \n",
       "3  tagged  undefined  undefined  undefined     undefined    undefined ...    \n",
       "4  tagged  undefined  undefined     Answer     undefined  Elaboration ...    \n",
       "\n",
       "  VBN VBP  VBZ  JJ JJR  JJS  RB  RBR  RBS  WRB  \n",
       "0   0   1    0   2   0    0   0    0    0    0  \n",
       "1   0   0    0   2   0    0   0    0    0    0  \n",
       "2   0   0    0   1   0    0   0    0    0    0  \n",
       "3   0   1    0   8   0    2   4    0    0    0  \n",
       "4   0   0    0   7   0    0   3    0    0    0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE = \"Help\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_tag(row):\n",
    "    h, p, a, c, e, j = row[5], row[6], row[7], row[8], row[9], row[10]\n",
    "    if CANDIDATE == 'Help':\n",
    "        if h == 'Help' or p == 'Problem':\n",
    "            return 'Help'\n",
    "        else:\n",
    "            return 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b538cec8ec0841c5a21fec2106a5eb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=59465), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['Tag'] = df.progress_apply(assign_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Last_Tag'] = df['Last_Tag'].fillna(\"NA\")\n",
    "df['Post'] = df['Post'].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered['Author'] = le.fit_transform(df_filtered.Author.values)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "leTag = preprocessing.LabelEncoder()\n",
    "df['Tag'] = leTag.fit_transform(df['Tag'].values)\n",
    "\n",
    "leType = preprocessing.LabelEncoder()\n",
    "df['Type'] = leType.fit_transform(df['Type'].values)\n",
    "\n",
    "leLastTag = preprocessing.LabelEncoder()\n",
    "df['Last_Tag'] = leLastTag.fit_transform(df['Last_Tag'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= [c for c in df.columns.values if c  not in ['Tag2', \\\n",
    "                                                      'date_feature', 'commas','stemmed', \\\n",
    "                                                       'Help','Problem','Answer','Clarification','Elaboration','Junk']]\n",
    "                                                               #'is_help','is_problem','is_announcement']]\n",
    "                                                               #'length','words','words_not_stopword','avg_word_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Title',\n",
       " 'Post',\n",
       " 'Type',\n",
       " 'Tag1',\n",
       " 'Original_post',\n",
       " 'count_post',\n",
       " 'Position',\n",
       " 'Last_Tag',\n",
       " 'count_help',\n",
       " 'count_wh',\n",
       " 'count_other',\n",
       " 'length',\n",
       " 'words_not_stopword',\n",
       " 'NN',\n",
       " 'NNS',\n",
       " 'NNP',\n",
       " 'NNPS',\n",
       " 'PRP',\n",
       " 'PRP$',\n",
       " 'WP',\n",
       " 'WP$',\n",
       " 'VB',\n",
       " 'VBD',\n",
       " 'VBG',\n",
       " 'VBN',\n",
       " 'VBP',\n",
       " 'VBZ',\n",
       " 'JJ',\n",
       " 'JJR',\n",
       " 'JJS',\n",
       " 'RB',\n",
       " 'RBR',\n",
       " 'RBS',\n",
       " 'WRB',\n",
       " 'Tag']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Title', 'Post', 'Type', 'Tag1', 'Original_post', 'count_post',\n",
       "       'Position', 'Last_Tag', 'count_help', 'count_wh', 'count_other',\n",
       "       'length', 'words_not_stopword', 'NN', 'NNS', 'NNP', 'NNPS', 'PRP',\n",
       "       'PRP$', 'WP', 'WP$', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ',\n",
       "       'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'WRB', 'Tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Post</th>\n",
       "      <th>Type</th>\n",
       "      <th>Tag1</th>\n",
       "      <th>Original_post</th>\n",
       "      <th>count_post</th>\n",
       "      <th>Position</th>\n",
       "      <th>Last_Tag</th>\n",
       "      <th>count_help</th>\n",
       "      <th>...</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJR</th>\n",
       "      <th>JJS</th>\n",
       "      <th>RB</th>\n",
       "      <th>RBR</th>\n",
       "      <th>RBS</th>\n",
       "      <th>WRB</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>2017 2 hello see announcement 2017 2 out machi...</td>\n",
       "      <td>1</td>\n",
       "      <td>tagged</td>\n",
       "      <td>2017.2 \" hello,    i didn't see any announceme...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>hi dave yes update available got it rosemarie</td>\n",
       "      <td>0</td>\n",
       "      <td>tagged</td>\n",
       "      <td>\" hi, dave,    yes, the update is available. i...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>change suspect rc1 canadate available day</td>\n",
       "      <td>0</td>\n",
       "      <td>tagged</td>\n",
       "      <td>it is out no change i suspect from the rc1 can...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>hi also updated latest nvda version help menu ...</td>\n",
       "      <td>0</td>\n",
       "      <td>tagged</td>\n",
       "      <td>\" hi!    i also updated to the latest nvda-ver...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2017.2</td>\n",
       "      <td>sorry folk mick jamie working later last night...</td>\n",
       "      <td>0</td>\n",
       "      <td>tagged</td>\n",
       "      <td>\"  sorry folks, mick and jamie were working la...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Title                                               Post  Type  \\\n",
       "0   1  2017.2  2017 2 hello see announcement 2017 2 out machi...     1   \n",
       "1   2  2017.2      hi dave yes update available got it rosemarie     0   \n",
       "2   3  2017.2          change suspect rc1 canadate available day     0   \n",
       "3   4  2017.2  hi also updated latest nvda version help menu ...     0   \n",
       "4   5  2017.2  sorry folk mick jamie working later last night...     0   \n",
       "\n",
       "     Tag1                                      Original_post  count_post  \\\n",
       "0  tagged  2017.2 \" hello,    i didn't see any announceme...           7   \n",
       "1  tagged  \" hi, dave,    yes, the update is available. i...           7   \n",
       "2  tagged  it is out no change i suspect from the rc1 can...           7   \n",
       "3  tagged  \" hi!    i also updated to the latest nvda-ver...           7   \n",
       "4  tagged  \"  sorry folks, mick and jamie were working la...           7   \n",
       "\n",
       "   Position  Last_Tag  count_help ...   VBP  VBZ  JJ  JJR  JJS  RB  RBR  RBS  \\\n",
       "0         1         6           0 ...     1    0   2    0    0   0    0    0   \n",
       "1         2         3           0 ...     0    0   2    0    0   0    0    0   \n",
       "2         4         0           0 ...     0    0   1    0    0   0    0    0   \n",
       "3         5         0           1 ...     1    0   8    0    2   4    0    0   \n",
       "4         7         4           0 ...     0    0   7    0    0   3    0    0   \n",
       "\n",
       "   WRB  Tag  \n",
       "0    0    0  \n",
       "1    0    1  \n",
       "2    0    1  \n",
       "3    0    1  \n",
       "4    0    1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<59465x47107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2379792 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "Post = Pipeline([\n",
    "                ('selector', TextSelector(key='Post')),\n",
    "                ('cnt', CountVectorizer()), #stop_words='english' \n",
    "                #('tf_idf', TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')),\n",
    "            ])\n",
    "\n",
    "Post.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Post_tf_idf = Pipeline([\n",
    "#                 ('selector', TextSelector(key='Post')),\n",
    "#                 #('cnt', CountVectorizer()), #stop_words='english' \n",
    "#                 ('tf_idf', TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')),\n",
    "#             ])\n",
    "\n",
    "# Post_tf_idf.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "count_other =  Pipeline([\n",
    "                ('selector', NumberSelector(key='count_other')),\n",
    "            ])\n",
    "count_help =  Pipeline([\n",
    "                ('selector', NumberSelector(key='count_help')),\n",
    "            ])\n",
    "count_wh =  Pipeline([\n",
    "                ('selector', NumberSelector(key='count_wh')),\n",
    "            ])\n",
    "\n",
    "count_other.fit_transform(df)\n",
    "count_help.fit_transform(df)\n",
    "count_wh.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "position =  Pipeline([\n",
    "                ('selector', NumberSelector(key='Position')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "last_tag =  Pipeline([\n",
    "                ('selector', NumberSelector(key='Last_Tag')),\n",
    "            ])\n",
    "\n",
    "typee =  Pipeline([\n",
    "                ('selector', NumberSelector(key='Type')),\n",
    "            ])\n",
    "\n",
    "position.fit_transform(df)\n",
    "last_tag.fit_transform(df)\n",
    "typee.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "length.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "RB_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='RB')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "RBR_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='RBR')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "RBS_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='RBS')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "WRB_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='WRB')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "RB_count.fit_transform(df)\n",
    "RBR_count.fit_transform(df)\n",
    "RBS_count.fit_transform(df)\n",
    "WRB_count.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "JJ_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='JJ')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "JJR_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='JJR')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "JJS_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='JJS')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "JJ_count.fit_transform(df)\n",
    "JJR_count.fit_transform(df)\n",
    "JJS_count.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "VB_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='VB')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "VBD_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='VBD')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "VBG_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='VBG')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "VBN_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='VBN')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "VBP_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='VBP')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "VBZ_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='VBZ')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "VB_count.fit_transform(df)\n",
    "VBD_count.fit_transform(df)\n",
    "VBG_count.fit_transform(df)\n",
    "VBN_count.fit_transform(df)\n",
    "VBP_count.fit_transform(df)\n",
    "VBZ_count.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "NN_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='NN')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "NNS_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='NNS')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "NNP_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='NNP')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "NNPS_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='NNPS')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "NN_count.fit_transform(df)\n",
    "NNS_count.fit_transform(df)\n",
    "NNP_count.fit_transform(df)\n",
    "NNPS_count.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "PRP_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='PRP')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "PRP__count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='PRP$')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "WP_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='WP')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "WP__count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='WP$')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "PRP_count.fit_transform(df)\n",
    "PRP__count.fit_transform(df)\n",
    "WP_count.fit_transform(df)\n",
    "WP__count.fit_transform(df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<59465x47135 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3782314 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('Post', Post), \n",
    "#                      ('Post_tf_idf', Post),\n",
    "                      ('length', length),\n",
    "                      ('count_help', count_help),\n",
    "                      ('count_other', count_other),\n",
    "                      ('count_wh', count_wh),\n",
    "#                       ('upper_case_word_count', upper_case_word_count),\n",
    "#                       ('words', words),\n",
    "#                       ('words_not_stopword', words_not_stopword),\n",
    "#                       ('avg_word_length', avg_word_length),\n",
    "#                       ('char_count', char_count),\n",
    "#                       ('word_count', word_count),\n",
    "#                       ('word_density', word_density), \n",
    "                      ('NN_count', NN_count),\n",
    "                      ('NNS_count', NNS_count),\n",
    "                      ('NNP_count', NNP_count),\n",
    "                      ('NNPS_count', NNPS_count),\n",
    "                      ('PRP_count', PRP_count),\n",
    "                      ('PRP$_count', PRP__count),\n",
    "                      ('WP_count', WP_count),\n",
    "                      ('WP$_count', WP__count),\n",
    "                      ('VB_count', VB_count),\n",
    "                      ('VBD_count', VBD_count),\n",
    "                      ('VBG_count', VBG_count),\n",
    "                      ('VBN_count', VBN_count),\n",
    "                      ('VBP_count', VBP_count),\n",
    "                      ('VBZ_count', VBZ_count),\n",
    "                      ('JJ_count', JJ_count),\n",
    "                      ('JJR_count', JJR_count),\n",
    "                      ('JJS_count', JJS_count),\n",
    "                      ('RB_count', RB_count),\n",
    "                      ('RBR_count', RBR_count),\n",
    "                      ('RBS_count', RBS_count),\n",
    "                      ('WRB_count', WRB_count),\n",
    "                      ('position', RBS_count),\n",
    "                      ('last_tag', WRB_count),\n",
    "                      ('Type', typee),\n",
    "                     ],\n",
    "                    # weight components in FeatureUnion\n",
    "            transformer_weights={\n",
    "            'Post': 0.8})\n",
    "                      \n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_eq_split(X, y, n_per_class, random_state=None):\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    sampled = X.groupby(y, sort=False).apply(\n",
    "        lambda frame: frame.sample(n_per_class))\n",
    "    mask = sampled.index.get_level_values(1)\n",
    "\n",
    "    X_train = X.drop(mask)\n",
    "    X_test = X.loc[mask]\n",
    "    y_train = y.drop(mask)\n",
    "    y_test = y.loc[mask]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Title', 'Post', 'Type', 'Tag1', 'Original_post', 'count_post',\n",
       "       'Position', 'Last_Tag', 'count_help', 'count_wh', 'count_other',\n",
       "       'length', 'words_not_stopword', 'NN', 'NNS', 'NNP', 'NNPS', 'PRP',\n",
       "       'PRP$', 'WP', 'WP$', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ',\n",
       "       'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'WRB', 'Tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59465\n",
      "4129\n",
      "55336\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df_tagged_data = df[df.Tag1 == 'tagged' ]\n",
    "print(len(df_tagged_data))\n",
    "df_final_data = df[df.Tag1 != 'tagged' ]\n",
    "print(len(df_final_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tagged_data.groupby('Tag')['Post'].count().plot.bar(ylim=0)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394\n"
     ]
    }
   ],
   "source": [
    "l = len(df_tagged_data.loc[df_tagged_data['Tag']==0])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_temp = df_tagged_data.groupby('Tag').head(l)\n",
    "df_tagged_data = df_temp\n",
    "\n",
    "df_tagged_data.groupby('Tag')['Post'].count().plot.bar(ylim=0)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df_tagged_data.drop(['Tag', 'Tag1'], axis = 1)\n",
    "y_data = df_tagged_data['Tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PER_CLASS = 60\n",
    "X_train, X_test, y_train, y_test = train_test_eq_split(X_data, y_data, n_per_class=N_PER_CLASS, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def print_scores(y_test, preds):\n",
    "    print(\"F1:\", f1_score(y_test, preds, average=\"macro\"))\n",
    "    print(\"Precision:\", precision_score(y_test, preds, average=\"macro\"))\n",
    "    print(\"Recall:\", recall_score(y_test, preds, average=\"macro\"))   \n",
    "    #print(\"AUC\", roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.824010056568196\n",
      "Precision: 0.8324808184143222\n",
      "Recall: 0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "score = np.mean(preds == y_test)\n",
    "\n",
    "print_scores(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9248695652173913\n",
      "Precision: 0.927972027972028\n",
      "Recall: 0.925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', LogisticRegression(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "score = np.mean(preds == y_test)\n",
    "\n",
    "print_scores(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Final Version Code/logistirregression.help.pickle.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2b1b75534ce2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Final Version Code/logistirregression.help.pickle.dat\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Final Version Code/logistirregression.help.pickle.dat'"
     ]
    }
   ],
   "source": [
    "# # save model to file\n",
    "import pickle \n",
    "\n",
    "pickle.dump(pipeline, open(\"../Final Version Code/logistirregression.help.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier',svm.SVC(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "score = np.mean(preds == y_test)\n",
    "\n",
    "print_scores(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier',svm.LinearSVC(random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "score = np.mean(preds == y_test)\n",
    "\n",
    "print_scores(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', xgboost.XGBClassifier(\n",
    "                                         subsample=0.8,\n",
    "                                         scale_pos_weight=1,\n",
    "                                         nthread=15,   \n",
    "                                         n_estimators=100,\n",
    "                                         min_child_weight=1,\n",
    "                                         max_depth=20,                                 \n",
    "                                         learning_rate =0.2,\n",
    "                                         gamma=.5,\n",
    "                                         colsample_bytree=0.5,\n",
    "                                         objective= 'binary:logistic',\n",
    "                                         seed=27,\n",
    "                                         eval_metric = 'error',\n",
    "                                         ))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "preds = pipeline.predict(X_test)\n",
    "score = np.mean(preds == y_test)\n",
    "\n",
    "print_scores(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "target = 'Tag'\n",
    "category_id_df = df_tagged_data[[target]].drop_duplicates().sort_values(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, preds)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r', \\\n",
    "           xticklabels=leTag.inverse_transform(category_id_df[target].values),\n",
    "           yticklabels=leTag.inverse_transform(category_id_df[target].values))\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "print(f1_score(y_test, preds, average=\"macro\"))\n",
    "print(precision_score(y_test, preds, average=\"macro\"))\n",
    "print(recall_score(y_test, preds, average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn import cross_validation\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "scores = cross_validation.cross_val_score(pipeline, X_data, y_data, cv=5)\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "        'classifier__learning_rate' : [0.1,0.2,0.05],\n",
    "        'classifier__subsample' : [0.5,0.6,0.7,0.8,0.9],\n",
    "        'classifier__colsample_bytree' : [0.5,0.6,0.7,0.8,0.9],\n",
    "        'classifier__nthread' : [10,12,15,20],\n",
    "        'classifier__scale_pos_weight' : [1,2,3,4,5],\n",
    "        'classifier__max_depth': [6, 10, 15, 1,2,3,4,5],\n",
    "        'classifier__gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'classifier__min_child_weight' : [1,2,3,4,5],\n",
    "        'classifier__n_estimators': [100,200,150]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gs = RandomizedSearchCV(pipeline, param_grid, n_jobs=1)  \n",
    "gs.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.predict(X_test)\n",
    "score = np.mean(preds == y_test)\n",
    "print_scores(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "\n",
    "X = X_data\n",
    "y = y_data\n",
    "\n",
    "scores_f1 = []\n",
    "scores_pre = []\n",
    "scores_rec = []\n",
    "scores_auc = []\n",
    "\n",
    "for i in range(5): \n",
    "    X_train, X_test, y_train, y_test = train_test_eq_split(X_data, y_data, n_per_class=5)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    preds = gs.predict(X_test)\n",
    "    #score = np.mean(preds == y_test)\n",
    "    f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    pr = precision_score(y_test, preds, average=\"macro\")\n",
    "    re = recall_score(y_test, preds, average=\"macro\")\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    scores_f1.append(f1)\n",
    "    scores_pre.append(pr)\n",
    "    scores_rec.append(re)\n",
    "    scores_auc.append(auc)\n",
    "    \n",
    "    print(\"Pre\" , pr)\n",
    "    print(\"Re:\" , re)\n",
    "    print(\"f1\" , f1)\n",
    "    print(\"AUC\" , auc)\n",
    "    print(\"-----\")\n",
    "    \n",
    "print(\"mean F1: \",  np.array(scores_f1).mean())\n",
    "print(\"mean Precission: \",  np.array(scores_pre).mean())\n",
    "print(\"mean Recall: \",  np.array(scores_rec).mean())\n",
    "print(\"mean auc_roc: \", np.array(scores_auc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "    \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = LinearSVC()\n",
    "X = X_data\n",
    "y = y_data\n",
    "\n",
    "scores_f1 = []\n",
    "scores_pre = []\n",
    "scores_rec = []\n",
    "scores_auc = []\n",
    "\n",
    "for i in range(5): \n",
    "    X_train, X_test, y_train, y_test = train_test_eq_split(X_data, y_data, n_per_class=5)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features',feats),\n",
    "        ('classifier',svm.LinearSVC(random_state = 42)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "    \n",
    "    #score = np.mean(preds == y_test)\n",
    "    f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    pr = precision_score(y_test, preds, average=\"macro\")\n",
    "    re = recall_score(y_test, preds, average=\"macro\")\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    scores_f1.append(f1)\n",
    "    scores_pre.append(pr)\n",
    "    scores_rec.append(re)\n",
    "    scores_auc.append(auc)\n",
    "    \n",
    "    print(\"Pre\" , pr)\n",
    "    print(\"Re:\" , re)\n",
    "    print(\"f1\" , f1)\n",
    "    print(\"AUC\" , auc)\n",
    "    print(\"-----\")\n",
    "    \n",
    "print(\"mean F1: \",  np.array(scores_f1).mean())\n",
    "print(\"mean Precission: \",  np.array(scores_pre).mean())\n",
    "print(\"mean Recall: \",  np.array(scores_rec).mean())\n",
    "print(\"mean auc_roc: \", np.array(scores_auc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "    \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = LinearSVC()\n",
    "X = X_data\n",
    "y = y_data\n",
    "\n",
    "scores_f1 = []\n",
    "scores_pre = []\n",
    "scores_rec = []\n",
    "scores_auc = []\n",
    "\n",
    "for i in range(5): \n",
    "    X_train, X_test, y_train, y_test = train_test_eq_split(X_data, y_data, n_per_class=5)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', LogisticRegression(random_state = 42)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "    \n",
    "    #score = np.mean(preds == y_test)\n",
    "    f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    pr = precision_score(y_test, preds, average=\"macro\")\n",
    "    re = recall_score(y_test, preds, average=\"macro\")\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    scores_f1.append(f1)\n",
    "    scores_pre.append(pr)\n",
    "    scores_rec.append(re)\n",
    "    scores_auc.append(auc)\n",
    "    \n",
    "    print(\"Pre\" , pr)\n",
    "    print(\"Re:\" , re)\n",
    "    print(\"f1\" , f1)\n",
    "    print(\"AUC\" , auc)\n",
    "    print(\"-----\")\n",
    "    \n",
    "print(\"mean F1: \",  np.array(scores_f1).mean())\n",
    "print(\"mean Precission: \",  np.array(scores_pre).mean())\n",
    "print(\"mean Recall: \",  np.array(scores_rec).mean())\n",
    "print(\"mean auc_roc: \", np.array(scores_auc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import h5py\n",
    "    \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X = X_data\n",
    "y = y_data\n",
    "\n",
    "scores_f1 = []\n",
    "scores_pre = []\n",
    "scores_rec = []\n",
    "scores_auc = []\n",
    "\n",
    "for i in range(5): \n",
    "    X_train, X_test, y_train, y_test = train_test_eq_split(X_data, y_data, n_per_class=5)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(random_state = 42)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "    \n",
    "    #score = np.mean(preds == y_test)\n",
    "    f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "    pr = precision_score(y_test, preds, average=\"macro\")\n",
    "    re = recall_score(y_test, preds, average=\"macro\")\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    scores_f1.append(f1)\n",
    "    scores_pre.append(pr)\n",
    "    scores_rec.append(re)\n",
    "    scores_auc.append(auc)\n",
    "    \n",
    "    print(\"Pre\" , pr)\n",
    "    print(\"Re:\" , re)\n",
    "    print(\"f1\" , f1)\n",
    "    print(\"AUC\" , auc)\n",
    "    print(\"-----\")\n",
    "    \n",
    "print(\"mean F1: \",  np.array(scores_f1).mean())\n",
    "print(\"mean Precission: \",  np.array(scores_pre).mean())\n",
    "print(\"mean Recall: \",  np.array(scores_rec).mean())\n",
    "print(\"mean auc_roc: \", np.array(scores_auc).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, preds)\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(fpr, tpr, marker='.')\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "print(roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_test)):\n",
    "    ac = list(y_test)[i]\n",
    "    pr = preds[i]\n",
    "    if ac!=pr:\n",
    "        print(ac)\n",
    "        print(X_test.Post.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "scores = cross_validation.cross_val_score(pipeline, X_data, y_data, cv=5)\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "X = X_data\n",
    "y = y_data\n",
    "\n",
    "scores = []\n",
    "for train_index, test_index in skf.split(X, y): \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index] \n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    preds = pipeline.predict(X_test)\n",
    "    score = np.mean(preds == y_test)\n",
    "    scores.append(score)\n",
    "\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df[df.Type==1]\n",
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load model from file\n",
    "loaded_model = pickle.load(open(\"../FInal Version Code/logistirregression.help.pickle.dat\", \"rb\"))\n",
    "# make predictions for test data\n",
    "y_pred = loaded_model.predict(final_df)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[20595:20605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Tag'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['ID', 'Title', 'Original_post', 'Tag']].to_csv('./Data/Help_prediction_v0_jaws.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

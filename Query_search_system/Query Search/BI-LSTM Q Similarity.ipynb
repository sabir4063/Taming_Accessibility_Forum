{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabir Ismail\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "'''Important Links - \n",
    "*** Understanding embedding layer in the model -\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "*** Understanding how to make lstm model in keras - \n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "*** I couldnt understand AdaDelta - \n",
    "https://www.quora.com/What-are-differences-between-update-rules-like-AdaDelta-RMSProp-AdaGrad-and-AdaM\n",
    "*** Why we need to clip the gradient descent\n",
    "http://nmarkou.blogspot.in/2017/07/deep-learning-why-you-should-use.html\n",
    "'''\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Merge\n",
    "import keras.layers as lyr\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Bidirectional\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "TRAIN_CSV = 'C:\\\\Users\\\\Sabir Ismail\\\\Downloads\\\\Quora Data Set\\\\train.csv'\n",
    "#TRAIN_CSV = 'sample.csv'\n",
    "EMBEDDING_FILE = 'C:\\\\Software\\\\Googlew2v\\\\GoogleNews-vectors-negative300.bin\\\\GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "\n",
    "def stringToWordList(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\(\", \" ( \", text)\n",
    "    text = re.sub(r\"\\)\", \" ) \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[17]:\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "vocab = dict()\n",
    "maxSeqLength = 0\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabir Ismail\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    for question in questions_cols:\n",
    "        q2num = []\n",
    "        for word in stringToWordList(row[question]):\n",
    "            if word in stops and word not in word2vec.vocab:\n",
    "                continue\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "            q2num.append(vocab[word])\n",
    "        train_df.set_value(index,question,q2num)\n",
    "        if len(q2num) > maxSeqLength:\n",
    "            maxSeqLength = len(q2num)\n",
    "\n",
    "embeddingDim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocab) + 1, embeddingDim)\n",
    "embeddings[0] = 0\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocab.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[20]:\n",
    "\n",
    "# Split the data into Training set and Validation set\n",
    "test_size = 20000\n",
    "validation_size = 40000\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X = train_df[questions_cols]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train_val, Y_train_val, test_size=validation_size)\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_test = {'left': X_test.question1, 'right': X_test.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_test = Y_test.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation, X_test], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=maxSeqLength)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabir Ismail\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "# In[21]:\n",
    "\n",
    "# Building the Siamese LSTM Model\n",
    "\n",
    "# Model variables\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 5\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(maxSeqLength,), dtype='int32')\n",
    "right_input = Input(shape=(maxSeqLength,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(embeddings), embeddingDim, weights=[embeddings], input_length=maxSeqLength, trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = Bidirectional(LSTM(n_hidden))\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Concatenate the embeddings with their product and squared difference.\n",
    "p = lyr.multiply([left_output, right_output])\n",
    "negative_right_output = lyr.Lambda(lambda x: -x)(right_output)\n",
    "d = lyr.add([left_output, negative_right_output])\n",
    "q = lyr.multiply([d, d])\n",
    "v = [left_output, right_output, p, q]\n",
    "lstm_output = lyr.concatenate(v)\n",
    "\n",
    "merged = lyr.BatchNormalization()(lstm_output)\n",
    "merged = lyr.Dense(64, activation='relu')(merged)\n",
    "merged = lyr.Dropout(0.2)(merged)\n",
    "merged = lyr.BatchNormalization()(merged)\n",
    "preds = lyr.Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(input=[left_input,right_input], output=preds)\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "model.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "training_start_time = time()\n",
    "\n",
    "lstm_trained = model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size,epochs=n_epoch,\n",
    "                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "\n",
    "print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 344290 samples, validate on 40000 samples\n",
      "Epoch 1/5\n",
      "344290/344290 [==============================] - 4517s 13ms/step - loss: 0.1578 - acc: 0.7680 - val_loss: 0.1395 - val_acc: 0.7990\n",
      "Epoch 2/5\n",
      "344290/344290 [==============================] - 4211s 12ms/step - loss: 0.1340 - acc: 0.8077 - val_loss: 0.1300 - val_acc: 0.8148\n",
      "Epoch 3/5\n",
      "344290/344290 [==============================] - 4543s 13ms/step - loss: 0.1238 - acc: 0.8248 - val_loss: 0.1244 - val_acc: 0.8250\n",
      "Epoch 4/5\n",
      "344290/344290 [==============================] - 4363s 13ms/step - loss: 0.1167 - acc: 0.8363 - val_loss: 0.1237 - val_acc: 0.8251\n",
      "Epoch 5/5\n",
      "344290/344290 [==============================] - 4354s 13ms/step - loss: 0.1115 - acc: 0.8447 - val_loss: 0.1191 - val_acc: 0.8313\n",
      "Training time finished.\n",
      "5 epochs in 6:06:31.434713\n",
      "acc: 83.33%\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate([X_test['left'], X_test['right']], Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate([X_test['left'], X_test['right']], Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV = 'C:\\\\Users\\\\Sabir Ismail\\\\Downloads\\\\Quora Data Set\\\\q_data.csv'\n",
    "\n",
    "test_df = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>a good program for reading pdf files</td>\n",
       "      <td>a decent program for perusing pdf records</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>A question about NVDA and Braille</td>\n",
       "      <td>An inquiry regarding NVDA and Braille</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>is it possible to keep the emoticons Always on?</td>\n",
       "      <td>how to keep emoticons always on</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Has any one done a factory reset on Windows 10...</td>\n",
       "      <td>how to do factory reset on windows</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>a bit off top question about free youtube to m...</td>\n",
       "      <td>how to convert youtube to mp3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2               a good program for reading pdf files   \n",
       "1   1     3     4                  A question about NVDA and Braille   \n",
       "2   2     5     6    is it possible to keep the emoticons Always on?   \n",
       "3   3     7     8  Has any one done a factory reset on Windows 10...   \n",
       "4   4     9    10  a bit off top question about free youtube to m...   \n",
       "\n",
       "                                   question2  is_duplicate  \n",
       "0  a decent program for perusing pdf records             1  \n",
       "1      An inquiry regarding NVDA and Braille             1  \n",
       "2            how to keep emoticons always on             1  \n",
       "3         how to do factory reset on windows             1  \n",
       "4              how to convert youtube to mp3             1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sabir Ismail\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "# In[17]:\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "vocab = dict()\n",
    "maxSeqLength = 0\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    for question in questions_cols:\n",
    "        q2num = []\n",
    "        for word in stringToWordList(row[question]):\n",
    "            if word in stops and word not in word2vec.vocab:\n",
    "                continue\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)+1\n",
    "            q2num.append(vocab[word])\n",
    "        test_df.set_value(index,question,q2num)\n",
    "        if len(q2num) > maxSeqLength:\n",
    "            maxSeqLength = len(q2num)\n",
    "\n",
    "embeddingDim = 300\n",
    "embeddings = 1 * np.random.randn(len(vocab) + 1, embeddingDim)\n",
    "embeddings[0] = 0\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocab.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embeddings[index] = word2vec.word_vec(word)\n",
    "\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_cols = ['question1', 'question2']\n",
    "test_X = test_df[questions_cols]\n",
    "test_Y = test_df['is_duplicate']\n",
    "\n",
    "test_X = {'left': test_X.question1, 'right': test_X.question2}\n",
    "print(len(test_X['left']))\n",
    "\n",
    "test_Y = test_Y.values\n",
    "print(len(test_Y))\n",
    "\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([test_X], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=212)\n",
    "\n",
    "print(len(test_X['left']))\n",
    "# # Make sure everything is ok\n",
    "assert test_X['left'].shape == test_X['right'].shape\n",
    "assert len(test_X['left']) == len(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "acc: 58.54%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "scores = loaded_model.evaluate([test_X['left'], test_X['right']], test_Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = loaded_model.predict([test_X['left'], test_X['right']]) \n",
    "y_classes = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
